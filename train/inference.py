import torch
import torch.nn.functional as F
import numpy as np
import cv2
import leap
import json
import os
import time
from typing import Dict, List, Optional
from collections import deque
from datetime import datetime

from model_definition import ModelFactory
from data_preprocessor import HandGesturePreprocessor


class GestureBuffer:
    """æ‰‹åŠ¿æ•°æ®ç¼“å†²å™¨"""

    def __init__(self, max_length: int = 30):
        self.max_length = max_length
        self.buffer = deque(maxlen=max_length)
        self.is_collecting = False
        self.last_hand_time = 0
        self.no_hand_duration = 1.0  # æ²¡æœ‰æ‰‹çš„æŒç»­æ—¶é—´

    def add_frame(self, frame_data: Dict) -> bool:
        """æ·»åŠ å¸§æ•°æ®ï¼Œè¿”å›æ˜¯å¦åº”è¯¥é¢„æµ‹"""
        current_time = time.time()
        has_hands = len(frame_data.get('hands', [])) > 0

        if has_hands:
            self.last_hand_time = current_time
            if not self.is_collecting:
                self.is_collecting = True
                self.buffer.clear()
                print(f"å¼€å§‹æ”¶é›†æ‰‹åŠ¿æ•°æ®...")
            self.buffer.append(frame_data)
            if len(self.buffer) % 5 == 0:  # æ¯5å¸§æ‰“å°ä¸€æ¬¡
                print(f"æ”¶é›†ä¸­... {len(self.buffer)}/{self.max_length} å¸§")
        else:
            if (self.is_collecting and
                    current_time - self.last_hand_time > self.no_hand_duration and
                    len(self.buffer) >= 10):

                self.is_collecting = False
                print(f"æ”¶é›†å®Œæˆ! å…±æ”¶é›† {len(self.buffer)} å¸§ï¼Œå‡†å¤‡é¢„æµ‹...")
                return True
            elif self.is_collecting:
                self.buffer.append(frame_data)
                print(f"æ‰‹åŠ¿ç»“æŸï¼Œç­‰å¾…é™æ­¢...")

        return False

    def get_sequence(self) -> List[Dict]:
        return list(self.buffer)


class HandGestureInference:
    """æ‰‹è¯­è¯†åˆ«æ¨ç†å¼•æ“"""

    def __init__(self, model_path: str, preprocessor_path: str):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

        # åŠ è½½é¢„å¤„ç†å™¨å’Œæ¨¡å‹
        self.preprocessor = HandGesturePreprocessor()
        self.data_splits = self.preprocessor.load_processed_data(preprocessor_path)

        # æ£€æŸ¥è®­ç»ƒæ•°æ®åˆ†å¸ƒ
        self._check_data_distribution()

        self.model = self._load_model(model_path)
        self.model.eval()

        # é¢„æµ‹æ§åˆ¶
        self.last_prediction_time = 0
        self.prediction_cooldown = 2.0

        print(f"æ¨ç†å¼•æ“åˆå§‹åŒ–å®Œæˆï¼Œä½¿ç”¨è®¾å¤‡: {self.device}")

    def _check_data_distribution(self):
        """æ£€æŸ¥è®­ç»ƒæ•°æ®åˆ†å¸ƒ"""
        print("\nğŸ“Š è®­ç»ƒæ•°æ®åˆ†å¸ƒæ£€æŸ¥:")

        if hasattr(self.preprocessor, 'label_decoder') and 'gesture' in self.preprocessor.label_decoder:
            gesture_decoder = self.preprocessor.label_decoder['gesture']
            print(f"  æ€»å…±æœ‰ {len(gesture_decoder)} ä¸ªæ‰‹åŠ¿ç±»åˆ«:")

            for idx, gesture in gesture_decoder.items():
                print(f"    ç±»åˆ«{idx}: {gesture}")

        # æ£€æŸ¥è®­ç»ƒé›†æ ‡ç­¾åˆ†å¸ƒ
        if 'y_gesture_train' in self.data_splits:
            y_train = self.data_splits['y_gesture_train']
            unique, counts = np.unique(y_train, return_counts=True)

            print(f"\n  è®­ç»ƒé›†æ ·æœ¬åˆ†å¸ƒ:")
            total_samples = len(y_train)
            for class_idx, count in zip(unique, counts):
                if class_idx in gesture_decoder:
                    gesture_name = gesture_decoder[class_idx]
                    percentage = (count / total_samples) * 100
                    print(f"    {gesture_name}: {count} æ ·æœ¬ ({percentage:.1f}%)")

            # æ£€æŸ¥æ•°æ®ä¸å¹³è¡¡
            min_count = np.min(counts)
            max_count = np.max(counts)
            imbalance_ratio = max_count / min_count

            print(f"\n  æ•°æ®å¹³è¡¡æ€§åˆ†æ:")
            print(f"    æœ€å°‘æ ·æœ¬: {min_count}")
            print(f"    æœ€å¤šæ ·æœ¬: {max_count}")
            print(f"    ä¸å¹³è¡¡æ¯”ç‡: {imbalance_ratio:.2f}")

            if imbalance_ratio > 5:
                print(f"    âš ï¸ ä¸¥é‡æ•°æ®ä¸å¹³è¡¡ï¼æŸäº›ç±»åˆ«æ ·æœ¬è¿‡å°‘å¯èƒ½å¯¼è‡´è¯†åˆ«åå‘å¤šæ•°ç±»åˆ«")
            elif imbalance_ratio > 2:
                print(f"    âš ï¸ è½»å¾®æ•°æ®ä¸å¹³è¡¡")
            else:
                print(f"    âœ… æ•°æ®åˆ†å¸ƒç›¸å¯¹å¹³è¡¡")

        print("-" * 50)

    def _load_model(self, model_path: str):
        """åŠ è½½æ¨¡å‹ - ä¿®å¤ç‰ˆæœ¬ï¼Œä¸ä¾èµ–é¢å¤–æ–¹æ³•"""
        print(f"æ­£åœ¨åŠ è½½æ¨¡å‹: {model_path}")

        checkpoint = torch.load(model_path, map_location=self.device, weights_only=False)
        model_type = checkpoint.get('model_type', 'lstm')
        print(f"æ¨¡å‹ç±»å‹: {model_type}")

        sample_data = self.data_splits['X_train']
        input_dim = sample_data.shape[-1]
        num_classes = len(self.preprocessor.label_decoder['gesture'])

        print(f"æ•°æ®è¾“å…¥ç»´åº¦: {input_dim}")
        print(f"ç±»åˆ«æ•°: {num_classes}")

        state_dict = checkpoint['model_state_dict']

        # ç›´æ¥åœ¨è¿™é‡Œæ¨æ–­é…ç½®ï¼Œä¸è°ƒç”¨é¢å¤–æ–¹æ³•
        config = {}

        try:
            if model_type == "lstm":
                # çº¯LSTMæ¨¡å‹
                if 'lstm.weight_ih_l0' in state_dict:
                    hidden_size = state_dict['lstm.weight_ih_l0'].shape[0] // 4
                    num_layers = 2 if 'lstm.weight_ih_l1' in state_dict else 1
                    bidirectional = 'lstm.weight_ih_l0_reverse' in state_dict

                    config = {
                        'hidden_dim': hidden_size,  # ä½¿ç”¨LSTMæ¨¡å‹çš„æ­£ç¡®å‚æ•°å
                        'num_layers': num_layers,
                        'bidirectional': bidirectional,
                        'dropout': 0.3
                    }
                    print(f"æ¨æ–­LSTMé…ç½®: {config}")

            elif model_type == "cnn_lstm":
                # CNN-LSTMæ¨¡å‹
                # CNNé…ç½®
                if 'cnn_extractor.cnn_layers.0.weight' in state_dict:
                    first_channels = state_dict['cnn_extractor.cnn_layers.0.weight'].shape[0]
                    second_channels = first_channels * 2
                    if 'cnn_extractor.cnn_layers.5.weight' in state_dict:
                        second_channels = state_dict['cnn_extractor.cnn_layers.5.weight'].shape[0]
                    config['cnn_channels'] = [first_channels, second_channels]

                # LSTMé…ç½®
                if 'lstm.weight_ih_l0' in state_dict:
                    hidden_size = state_dict['lstm.weight_ih_l0'].shape[0] // 4
                    config['lstm_hidden_size'] = hidden_size  # CNN-LSTMä½¿ç”¨è¿™ä¸ªå‚æ•°å
                    config['lstm_num_layers'] = 2 if 'lstm.weight_ih_l1' in state_dict else 1
                    config['bidirectional'] = 'lstm.weight_ih_l0_reverse' in state_dict

                # åˆ†ç±»å™¨é…ç½®
                if 'classifier.1.weight' in state_dict:
                    config['classifier_hidden_size'] = state_dict['classifier.1.weight'].shape[0]

                print(f"æ¨æ–­CNN-LSTMé…ç½®: {config}")

            else:
                print(f"ä½¿ç”¨é»˜è®¤é…ç½®åˆ›å»º {model_type} æ¨¡å‹")

        except Exception as e:
            print(f"æ¨æ–­é…ç½®æ—¶å‡ºé”™: {e}ï¼Œå°†ä½¿ç”¨é»˜è®¤é…ç½®")
            config = {}

        # åˆ›å»ºæ¨¡å‹
        try:
            model = ModelFactory.create_model(
                model_type,
                input_dim=input_dim,
                num_classes=num_classes,
                **config
            )
            print("âœ… æ¨¡å‹åˆ›å»ºæˆåŠŸ")

        except Exception as e:
            print(f"ä½¿ç”¨æ¨æ–­é…ç½®åˆ›å»ºæ¨¡å‹å¤±è´¥: {e}")
            print("å°è¯•ä½¿ç”¨é»˜è®¤é…ç½®...")
            model = ModelFactory.create_model(model_type, input_dim=input_dim, num_classes=num_classes)

        # åŠ è½½æƒé‡
        try:
            model.load_state_dict(checkpoint['model_state_dict'], strict=True)
            print("âœ… æ¨¡å‹æƒé‡ä¸¥æ ¼åŠ è½½æˆåŠŸ")
        except RuntimeError as e:
            print(f"ä¸¥æ ¼åŠ è½½å¤±è´¥: {e}")
            print("å°è¯•éä¸¥æ ¼åŠ è½½...")
            missing_keys, unexpected_keys = model.load_state_dict(
                checkpoint['model_state_dict'], strict=False
            )

            if missing_keys:
                print(f"ç¼ºå¤±å‚æ•°: {len(missing_keys)} ä¸ª")
                if len(missing_keys) <= 5:
                    print(f"  å…·ä½“: {missing_keys}")
            if unexpected_keys:
                print(f"å¤šä½™å‚æ•°: {len(unexpected_keys)} ä¸ª")
                if len(unexpected_keys) <= 5:
                    print(f"  å…·ä½“: {unexpected_keys}")

            # æ£€æŸ¥å…³é”®å‚æ•°æ˜¯å¦ç¼ºå¤±
            critical_missing = [key for key in missing_keys
                                if any(critical in key for critical in ['cnn', 'lstm', 'classifier'])]
            if critical_missing:
                print(f"âŒ å…³é”®å±‚å‚æ•°ç¼ºå¤±: {critical_missing}")
                raise RuntimeError(f"æ— æ³•åŠ è½½æ¨¡å‹ï¼Œå…³é”®å‚æ•°ç¼ºå¤±")

            print("âš ï¸ éƒ¨åˆ†å‚æ•°åŠ è½½æˆåŠŸï¼Œç»§ç»­è¿è¡Œ...")

        model.to(self.device)
        return model

    def extract_hand_features(self, hand_data: Dict) -> np.ndarray:
        """æå–æ‰‹éƒ¨ç‰¹å¾"""
        features = []

        # åŸºæœ¬ç‰¹å¾
        features.extend([
            1.0 if hand_data.get("hand_type") == "right" else 0.0,
            hand_data.get("confidence", 0),
            hand_data.get("grab_strength", 0),
            hand_data.get("grab_angle", 0),
            hand_data.get("pinch_distance", 0),
            hand_data.get("pinch_strength", 0)
        ])

        # æ‰‹æŒç‰¹å¾
        palm = hand_data.get("palm", {})
        features.extend(palm.get("position", [0, 0, 0]))
        features.extend(palm.get("direction", [0, 0, 0]))
        features.extend(palm.get("normal", [0, 0, 0]))
        features.extend(palm.get("velocity", [0, 0, 0]))
        features.append(palm.get("width", 0))

        # æ‰‹è‡‚ç‰¹å¾
        arm = hand_data.get("arm", {})
        features.extend(arm.get("prev_joint", [0, 0, 0]))
        features.extend(arm.get("next_joint", [0, 0, 0]))
        features.extend(arm.get("direction", [0, 0, 0]))
        features.append(arm.get("length", 0))
        features.append(arm.get("width", 0))

        # æ‰‹æŒ‡ç‰¹å¾
        digits = hand_data.get("digits", [])
        for digit_idx in range(5):
            if digit_idx < len(digits):
                digit = digits[digit_idx]
                features.append(float(digit.get("is_extended", True)))

                bones = digit.get("bones", [])
                for bone_idx in range(4):
                    if bone_idx < len(bones):
                        bone = bones[bone_idx]
                        features.extend(bone.get("prev_joint", [0, 0, 0]))
                        features.extend(bone.get("next_joint", [0, 0, 0]))
                        features.extend(bone.get("direction", [0, 0, 0]))
                        features.append(bone.get("length", 0))
                        features.append(bone.get("width", 0))
                    else:
                        features.extend([0] * 11)
            else:
                features.append(1.0)
                features.extend([0] * 44)

        return np.array(features[:200])  # é™åˆ¶ç‰¹å¾é•¿åº¦

    def extract_frame_features(self, frame_data: Dict) -> np.ndarray:
        """æå–å¸§ç‰¹å¾"""
        hands = frame_data.get("hands", [])

        # åŒæ‰‹ç‰¹å¾
        left_hand_features = np.zeros(200)
        right_hand_features = np.zeros(200)

        for hand in hands:
            hand_features = self.extract_hand_features(hand)
            hand_features = np.pad(hand_features, (0, max(0, 200 - len(hand_features))))[:200]

            if hand.get("hand_type") == "left":
                left_hand_features = hand_features
            else:
                right_hand_features = hand_features

        # åˆå¹¶ç‰¹å¾
        features = np.concatenate([left_hand_features, right_hand_features, [len(hands)]])
        return features

    def predict_gesture(self, sequence_data: List[Dict]) -> Optional[Dict]:
        """é¢„æµ‹æ‰‹è¯­"""
        print(f"å¼€å§‹é¢„æµ‹ï¼Œåºåˆ—é•¿åº¦: {len(sequence_data)}")

        if len(sequence_data) < 10:
            print(f"åºåˆ—å¤ªçŸ­: {len(sequence_data)} < 10")
            return None

        try:
            # æå–ç‰¹å¾åºåˆ—
            features = []
            for i, frame in enumerate(sequence_data):
                frame_features = self.extract_frame_features(frame)
                features.append(frame_features)
                if i < 3:  # åªæ‰“å°å‰3å¸§çš„ä¿¡æ¯
                    hands_count = len(frame.get('hands', []))
                    print(f"å¸§{i}: æ£€æµ‹åˆ°{hands_count}åªæ‰‹, ç‰¹å¾ç»´åº¦: {len(frame_features)}")

            # æ ‡å‡†åŒ–åºåˆ—é•¿åº¦åˆ°30å¸§
            features = np.array(features)
            print(f"åŸå§‹ç‰¹å¾å½¢çŠ¶: {features.shape}")

            target_length = 30

            if len(features) < target_length:
                padding = np.repeat(features[-1:], target_length - len(features), axis=0)
                features = np.vstack([features, padding])
                print(f"åºåˆ—è¿‡çŸ­ï¼Œå¡«å……åˆ°: {features.shape}")
            elif len(features) > target_length:
                indices = np.linspace(0, len(features) - 1, target_length, dtype=int)
                features = features[indices]
                print(f"åºåˆ—è¿‡é•¿ï¼Œé‡‡æ ·åˆ°: {features.shape}")

            # æ•°æ®ç¼©æ”¾
            original_shape = features.shape
            features_reshaped = features.reshape(-1, features.shape[-1])
            print(f"ç¼©æ”¾å‰å½¢çŠ¶: {features_reshaped.shape}")

            features_scaled = self.preprocessor.scaler.transform(features_reshaped)
            features = features_scaled.reshape(original_shape)
            print(f"ç¼©æ”¾åå½¢çŠ¶: {features.shape}")

            # æ¨¡å‹é¢„æµ‹
            input_tensor = torch.FloatTensor(features).unsqueeze(0).to(self.device)
            print(f"è¾“å…¥tensorå½¢çŠ¶: {input_tensor.shape}")

            with torch.no_grad():
                outputs = self.model(input_tensor)
                probs = F.softmax(outputs, dim=1)
                pred_idx = torch.argmax(probs, dim=1).item()
                confidence = torch.max(probs).item()

                print(f"æ¨¡å‹è¾“å‡ºåˆ†æ:")
                print(f"  é¢„æµ‹ç±»åˆ«: {pred_idx}")
                print(f"  æœ€é«˜ç½®ä¿¡åº¦: {confidence:.3f}")

                # æ˜¾ç¤ºæ‰€æœ‰ç±»åˆ«çš„æ¦‚ç‡åˆ†å¸ƒ
                print(f"  ç±»åˆ«æ¦‚ç‡åˆ†å¸ƒ:")
                probs_np = probs[0].cpu().numpy()
                for i, prob in enumerate(probs_np):
                    if i in self.preprocessor.label_decoder['gesture']:
                        gesture_name = self.preprocessor.label_decoder['gesture'][i]
                        marker = " â† é¢„æµ‹" if i == pred_idx else ""
                        print(f"    ç±»åˆ«{i}({gesture_name}): {prob:.4f}{marker}")

                # å¯¹äº2åˆ†ç±»é—®é¢˜ï¼Œæ£€æŸ¥æ˜¯å¦æœ‰æ˜ç¡®çš„åŒºåˆ†
                if len(probs_np) == 2:
                    prob_diff = abs(probs_np[0] - probs_np[1])
                    print(f"  ä¸¤ç±»æ¦‚ç‡å·®å€¼: {prob_diff:.4f}")
                    if prob_diff < 0.2:
                        print(f"    âš ï¸ ä¸¤ç±»æ¦‚ç‡å¾ˆæ¥è¿‘ï¼Œæ¨¡å‹ä¸ç¡®å®š")
                    elif prob_diff > 0.6:
                        print(f"    âœ… é¢„æµ‹å¾ˆç¡®å®š")
                    else:
                        print(f"    ğŸ”¶ é¢„æµ‹è¾ƒä¸ºç¡®å®š")

                # æ£€æŸ¥æ¦‚ç‡åˆ†å¸ƒæ˜¯å¦æ­£å¸¸
                prob_std = np.std(probs_np)
                if prob_std < 0.05:
                    print(f"    âš ï¸ è­¦å‘Š: æ¦‚ç‡åˆ†å¸ƒè¿‡äºå¹³å‡ï¼Œæ¨¡å‹å¯èƒ½æ²¡å­¦åˆ°åŒºåˆ«")

                print(f"    æ¦‚ç‡æ ‡å‡†å·®: {prob_std:.4f}")

            gesture_label = self.preprocessor.label_decoder['gesture'][pred_idx]
            print(f"é¢„æµ‹çš„æ‰‹åŠ¿æ ‡ç­¾: {gesture_label}")

            # è·å–ä¸­è‹±æ–‡å«ä¹‰
            gesture_labels_path = os.path.join(self.preprocessor.data_dir, "gesture_labels.json")
            chinese_meaning = "æœªçŸ¥"
            english_meaning = "unknown"

            if os.path.exists(gesture_labels_path):
                with open(gesture_labels_path, 'r', encoding='utf-8') as f:
                    gesture_labels = json.load(f)
                    gesture_info = gesture_labels.get(gesture_label, {})
                    chinese_meaning = gesture_info.get('chinese', 'æœªçŸ¥')
                    english_meaning = gesture_info.get('english', 'unknown')
                    print(f"æ ‡ç­¾æ˜ å°„: {gesture_label} -> {chinese_meaning}/{english_meaning}")
            else:
                print("è­¦å‘Š: gesture_labels.json æ–‡ä»¶ä¸å­˜åœ¨")

            return {
                'gesture_label': gesture_label,
                'chinese_meaning': chinese_meaning,
                'english_meaning': english_meaning,
                'confidence': confidence,
                'timestamp': datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            }

        except Exception as e:
            print(f"é¢„æµ‹å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            return None


class RealTimeGestureRecognizer:
    """å®æ—¶æ‰‹è¯­è¯†åˆ«å™¨"""

    def __init__(self, model_path: str, preprocessor_path: str):
        self.inference_engine = HandGestureInference(model_path, preprocessor_path)
        self.gesture_buffer = GestureBuffer()

        # ç•Œé¢è®¾ç½®
        self.screen_size = [600, 800]
        self.output_image = np.zeros((self.screen_size[0], self.screen_size[1], 3), np.uint8)

        # è¯†åˆ«çŠ¶æ€
        self.current_result = None
        self.result_display_time = 5.0
        self.result_start_time = 0
        self.recognition_status = "WAITING"  # WAITING, COLLECTING, SUCCESS, FAILED

        # ç»Ÿè®¡
        self.total_attempts = 0
        self.successful_recognitions = 0

    def process_prediction(self, frame_data: Dict) -> Optional[Dict]:
        """å¤„ç†é¢„æµ‹"""
        current_time = time.time()

        # æ£€æŸ¥å†·å´æ—¶é—´
        if current_time - self.inference_engine.last_prediction_time < self.inference_engine.prediction_cooldown:
            return None

        # æ·»åŠ åˆ°ç¼“å†²åŒº
        should_predict = self.gesture_buffer.add_frame(frame_data)

        if should_predict:
            sequence = self.gesture_buffer.get_sequence()
            if sequence:
                self.total_attempts += 1
                result = self.inference_engine.predict_gesture(sequence)

                if result and result['confidence'] > 0.4:  # è°ƒæ•´ä¸ºæ›´åˆç†çš„é˜ˆå€¼
                    self.successful_recognitions += 1
                    self.inference_engine.last_prediction_time = current_time

                    print(f"\nâœ… è¯†åˆ«æˆåŠŸ!")
                    print(f"æ‰‹åŠ¿: {result['gesture_label']}")
                    print(f"ä¸­æ–‡: {result['chinese_meaning']}")
                    print(f"è‹±æ–‡: {result['english_meaning']}")
                    print(f"ç½®ä¿¡åº¦: {result['confidence']:.3f}")
                    print("-" * 40)

                    return result
                else:
                    confidence_str = f"{result['confidence']:.3f}" if result else "æ— é¢„æµ‹ç»“æœ"
                    print(f"âŒ è¯†åˆ«å¤±è´¥: ç½®ä¿¡åº¦{confidence_str} (éœ€è¦>0.4)")

                    # æ˜¾ç¤ºå¤‡é€‰é¢„æµ‹
                    if result:
                        print(f"   å½“å‰é¢„æµ‹: {result['chinese_meaning']}({result['english_meaning']})")

                    return {"status": "failed", "reason": f"ç½®ä¿¡åº¦{confidence_str}"}

        return None

    def render_interface(self, event):
        """æ¸²æŸ“ç•Œé¢"""
        # æ¸…ç©ºç”»å¸ƒ
        self.output_image[:] = 0

        # æ ‡é¢˜
        cv2.putText(self.output_image, "Sign Language Recognition",
                    (20, 40), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)

        # ç»Ÿè®¡ä¿¡æ¯
        y = 80
        success_rate = (self.successful_recognitions / max(1, self.total_attempts) * 100)
        stats = [
            f"Attempts: {self.total_attempts}",
            f"Success: {self.successful_recognitions}",
            f"Rate: {success_rate:.1f}%",
            f"Buffer: {len(self.gesture_buffer.buffer)}/{self.gesture_buffer.max_length}"
        ]

        for stat in stats:
            cv2.putText(self.output_image, stat, (20, y), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
            y += 25

        # çŠ¶æ€æ˜¾ç¤ºåŒºåŸŸ
        status_y = y + 20
        status_height = 200

        # çŠ¶æ€é¢œè‰²
        colors = {
            "SUCCESS": (0, 255, 0),
            "FAILED": (0, 0, 255),
            "COLLECTING": (0, 255, 255),
            "WAITING": (128, 128, 128)
        }
        color = colors.get(self.recognition_status, (128, 128, 128))

        # çŠ¶æ€æ¡†
        cv2.rectangle(self.output_image, (20, status_y), (self.screen_size[1] - 20, status_y + status_height), color, 2)

        # çŠ¶æ€å†…å®¹
        if self.recognition_status == "SUCCESS" and self.current_result:
            cv2.putText(self.output_image, "RECOGNITION SUCCESS!",
                        (30, status_y + 40), cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 2)

            cv2.putText(self.output_image, f"Gesture: {self.current_result['gesture_label']}",
                        (30, status_y + 70), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)

            cv2.putText(self.output_image, f"Chinese: {self.current_result['chinese_meaning']}",
                        (30, status_y + 100), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)

            cv2.putText(self.output_image, f"English: {self.current_result['english_meaning']}",
                        (30, status_y + 130), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)

            cv2.putText(self.output_image, f"Confidence: {self.current_result['confidence']:.2f}",
                        (30, status_y + 160), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)

        elif self.recognition_status == "FAILED":
            cv2.putText(self.output_image, "RECOGNITION FAILED!",
                        (30, status_y + 40), cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 2)
            cv2.putText(self.output_image, "Try making a clearer gesture",
                        (30, status_y + 80), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)

        elif self.recognition_status == "COLLECTING":
            cv2.putText(self.output_image, "COLLECTING GESTURE...",
                        (30, status_y + 40), cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 2)

            # è¿›åº¦æ¡
            progress = len(self.gesture_buffer.buffer) / self.gesture_buffer.max_length
            bar_width = int((self.screen_size[1] - 60) * progress)
            cv2.rectangle(self.output_image, (30, status_y + 70), (30 + bar_width, status_y + 90), (0, 255, 255), -1)
            cv2.rectangle(self.output_image, (30, status_y + 70), (self.screen_size[1] - 30, status_y + 90),
                          (255, 255, 255), 1)

            cv2.putText(self.output_image, f"Progress: {progress * 100:.0f}%",
                        (30, status_y + 110), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)

        else:  # WAITING
            cv2.putText(self.output_image, "WAITING FOR GESTURE...",
                        (30, status_y + 40), cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 2)
            cv2.putText(self.output_image, "Place hand above sensor",
                        (30, status_y + 80), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)

        # ç»˜åˆ¶æ‰‹éƒ¨éª¨æ¶
        if hasattr(event, 'hands') and event.hands:
            if self.recognition_status == "WAITING":
                self.recognition_status = "COLLECTING"

            for hand in event.hands:
                self._draw_hand_skeleton(hand)
        else:
            # çŠ¶æ€è½¬æ¢é€»è¾‘
            current_time = time.time()
            if (self.recognition_status in ["SUCCESS", "FAILED"] and
                    current_time - self.result_start_time > self.result_display_time):
                self.recognition_status = "WAITING"
                self.current_result = None

        # æ§åˆ¶è¯´æ˜
        cv2.putText(self.output_image, "Press 'q' to quit",
                    (20, self.screen_size[0] - 20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 255), 1)

    def _draw_hand_skeleton(self, hand):
        """ç»˜åˆ¶æ‰‹éƒ¨éª¨æ¶"""
        try:
            # æ‰‹è‡‚
            wrist = self._get_joint_position(hand.arm.next_joint)
            elbow = self._get_joint_position(hand.arm.prev_joint)

            if wrist and elbow:
                cv2.line(self.output_image, wrist, elbow, (255, 255, 255), 2)
                cv2.circle(self.output_image, wrist, 4, (255, 255, 255), -1)
                cv2.circle(self.output_image, elbow, 4, (255, 255, 255), -1)

            # æ‰‹æŒ‡
            for digit in hand.digits:
                for bone in digit.bones:
                    start = self._get_joint_position(bone.prev_joint)
                    end = self._get_joint_position(bone.next_joint)

                    if start and end:
                        cv2.line(self.output_image, start, end, (255, 255, 255), 2)
                        cv2.circle(self.output_image, start, 3, (255, 255, 255), -1)
                        cv2.circle(self.output_image, end, 3, (255, 255, 255), -1)
        except:
            pass

    def _get_joint_position(self, joint):
        """è·å–å…³èŠ‚å±å¹•ä½ç½®"""
        if joint:
            x = int(joint.x + (self.screen_size[1] / 2))
            y = int(joint.z + (self.screen_size[0] / 2))
            return (x, y)
        return None

    def update_result(self, result):
        """æ›´æ–°è¯†åˆ«ç»“æœ"""
        if result:
            if result.get("status") == "failed":
                self.recognition_status = "FAILED"
            else:
                self.recognition_status = "SUCCESS"
                self.current_result = result
            self.result_start_time = time.time()


class GestureRecognitionListener(leap.Listener):
    """Leap Motionç›‘å¬å™¨"""

    def __init__(self, recognizer: RealTimeGestureRecognizer):
        self.recognizer = recognizer

    def on_connection_event(self, event):
        print("Leap Motionè¿æ¥æˆåŠŸ")

    def on_tracking_event(self, event):
        try:
            # æ¸²æŸ“ç•Œé¢
            self.recognizer.render_interface(event)

            # æ„å»ºå¸§æ•°æ®
            frame_data = {'timestamp': time.time(), 'hands': []}

            # æå–æ‰‹éƒ¨æ•°æ®
            if hasattr(event, 'hands') and event.hands:
                for hand in event.hands:
                    hand_data = self._extract_hand_data(hand)
                    if hand_data:
                        frame_data['hands'].append(hand_data)

            # å¤„ç†é¢„æµ‹
            result = self.recognizer.process_prediction(frame_data)
            if result:
                self.recognizer.update_result(result)

        except Exception as e:
            print(f"å¤„ç†è·Ÿè¸ªäº‹ä»¶å‡ºé”™: {e}")

    def _extract_hand_data(self, hand):
        """æå–æ‰‹éƒ¨æ•°æ®"""
        try:
            return {
                "hand_type": "left" if hasattr(hand, 'type') and hand.type == leap.HandType.Left else "right",
                "confidence": getattr(hand, 'confidence', 1.0),
                "grab_strength": getattr(hand, 'grab_strength', 0.0),
                "grab_angle": getattr(hand, 'grab_angle', 0.0),
                "pinch_distance": getattr(hand, 'pinch_distance', 0.0),
                "pinch_strength": getattr(hand, 'pinch_strength', 0.0),
                "palm": {
                    "position": self._get_vector3(getattr(getattr(hand, 'palm', None), 'position', None)),
                    "direction": self._get_vector3(getattr(getattr(hand, 'palm', None), 'direction', None)),
                    "normal": self._get_vector3(getattr(getattr(hand, 'palm', None), 'normal', None)),
                    "velocity": self._get_vector3(getattr(getattr(hand, 'palm', None), 'velocity', None)),
                    "width": getattr(getattr(hand, 'palm', None), 'width', 0.0)
                },
                "arm": {
                    "prev_joint": self._get_vector3(getattr(getattr(hand, 'arm', None), 'prev_joint', None)),
                    "next_joint": self._get_vector3(getattr(getattr(hand, 'arm', None), 'next_joint', None)),
                    "direction": self._get_vector3(getattr(getattr(hand, 'arm', None), 'direction', None)),
                    "length": getattr(getattr(hand, 'arm', None), 'length', 0.0),
                    "width": getattr(getattr(hand, 'arm', None), 'width', 0.0)
                },
                "digits": self._extract_digits(hand)
            }
        except Exception as e:
            print(f"æå–æ‰‹éƒ¨æ•°æ®å‡ºé”™: {e}")
            return None

    def _extract_digits(self, hand):
        """æå–æ‰‹æŒ‡æ•°æ®"""
        digits = []
        try:
            if hasattr(hand, 'digits') and hand.digits:
                for digit_idx in range(min(5, len(hand.digits))):
                    digit = hand.digits[digit_idx]
                    digit_data = {
                        "digit_type": digit_idx,
                        "is_extended": getattr(digit, 'is_extended', True),
                        "bones": []
                    }

                    if hasattr(digit, 'bones') and digit.bones:
                        for bone_idx in range(min(4, len(digit.bones))):
                            bone = digit.bones[bone_idx]
                            bone_data = {
                                "bone_type": bone_idx,
                                "prev_joint": self._get_vector3(getattr(bone, 'prev_joint', None)),
                                "next_joint": self._get_vector3(getattr(bone, 'next_joint', None)),
                                "direction": self._get_vector3(getattr(bone, 'direction', None)),
                                "length": getattr(bone, 'length', 0.0),
                                "width": getattr(bone, 'width', 0.0)
                            }
                            digit_data["bones"].append(bone_data)

                    # è¡¥é½4ä¸ªéª¨éª¼
                    while len(digit_data["bones"]) < 4:
                        digit_data["bones"].append({
                            "bone_type": len(digit_data["bones"]),
                            "prev_joint": [0.0, 0.0, 0.0],
                            "next_joint": [0.0, 0.0, 0.0],
                            "direction": [0.0, 0.0, 0.0],
                            "length": 0.0,
                            "width": 0.0
                        })

                    digits.append(digit_data)

            # è¡¥é½5ä¸ªæ‰‹æŒ‡
            while len(digits) < 5:
                digits.append({
                    "digit_type": len(digits),
                    "is_extended": True,
                    "bones": [{
                        "bone_type": i,
                        "prev_joint": [0.0, 0.0, 0.0],
                        "next_joint": [0.0, 0.0, 0.0],
                        "direction": [0.0, 0.0, 0.0],
                        "length": 0.0,
                        "width": 0.0
                    } for i in range(4)]
                })

        except Exception as e:
            print(f"æå–æ‰‹æŒ‡æ•°æ®å‡ºé”™: {e}")

        return digits

    def _get_vector3(self, vector):
        """å®‰å…¨è·å–3Då‘é‡"""
        if vector is None:
            return [0.0, 0.0, 0.0]
        try:
            if hasattr(vector, 'x') and hasattr(vector, 'y') and hasattr(vector, 'z'):
                return [float(vector.x), float(vector.y), float(vector.z)]
        except:
            pass
        return [0.0, 0.0, 0.0]


def main():
    """ä¸»å‡½æ•°"""
    print("å¯åŠ¨æ‰‹è¯­è¯†åˆ«ç³»ç»Ÿ")
    print("=" * 30)

    # æŸ¥æ‰¾æ–‡ä»¶
    models_dir = "data/models"
    processed_dir = "data/processed"

    model_path = None
    if os.path.exists(models_dir):
        model_files = [f for f in os.listdir(models_dir) if f.endswith('.pth')]
        if model_files:
            model_path = os.path.join(models_dir, model_files[0])

    preprocessor_path = None
    if os.path.exists(processed_dir):
        processed_files = [f for f in os.listdir(processed_dir) if f.endswith('.pkl')]
        if processed_files:
            preprocessor_path = os.path.join(processed_dir, sorted(processed_files)[-1])

    if not model_path or not preprocessor_path:
        print("ç¼ºå°‘æ¨¡å‹æˆ–æ•°æ®æ–‡ä»¶ï¼Œè¯·å…ˆè¿è¡Œè®­ç»ƒæµç¨‹")
        return

    print(f"æ¨¡å‹: {os.path.basename(model_path)}")
    print(f"æ•°æ®: {os.path.basename(preprocessor_path)}")

    try:
        # åˆ›å»ºè¯†åˆ«å™¨
        recognizer = RealTimeGestureRecognizer(model_path, preprocessor_path)
        listener = GestureRecognitionListener(recognizer)

        connection = leap.Connection()
        connection.add_listener(listener)

        print("ç³»ç»Ÿå¯åŠ¨æˆåŠŸ! æŒ‰'q'é€€å‡º")

        with connection.open():
            connection.set_tracking_mode(leap.TrackingMode.Desktop)

            while True:
                cv2.imshow("Sign Language Recognition", recognizer.output_image)
                if cv2.waitKey(1) & 0xFF == ord('q'):
                    break

        cv2.destroyAllWindows()

        # æ˜¾ç¤ºç»Ÿè®¡
        print(f"\nç»Ÿè®¡: å°è¯•{recognizer.total_attempts}æ¬¡, "
              f"æˆåŠŸ{recognizer.successful_recognitions}æ¬¡, "
              f"æˆåŠŸç‡{(recognizer.successful_recognitions / max(1, recognizer.total_attempts) * 100):.1f}%")

    except Exception as e:
        print(f"å¯åŠ¨å¤±è´¥: {e}")


if __name__ == "__main__":
    main()